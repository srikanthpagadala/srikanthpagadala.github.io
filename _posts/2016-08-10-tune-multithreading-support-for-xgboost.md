---
layout: post
title: "Tune Multithreading Support for XGBoost"
date: 2016-08-10
categories: ['Machine Learning']
---

The XGBoost library for gradient boosting is designed for efficient multi-core parallel processing.

This allows it to efficiently use all of the CPU cores in your system when training.

Here you will discover the parallel processing capabilities of the XGBoost.

By the end you will know:

- How to confirm that XGBoost multi-threading support is working on your system?
- How to evaluate the effect of increasing the number of threads on XGBoost?
- How to get the most out of multithreaded XGBoost when using cross validation and grid search?

[Source Code](https://github.com/srikanthpagadala/machine-learning-projects/tree/master/Tune%20Multithreading%20Support%20for%20XGBoost){:target="_blank"}

[Report](http://htmlpreview.github.io/?https://github.com/srikanthpagadala/machine-learning-projects/blob/master/Tune%20Multithreading%20Support%20for%20XGBoost/report.html){:target="_blank"}

Next: [Tune the Number and Size of Decision Trees with XGBoost](/notes/2016/08/11/tune-the-number-and-size-of-decision-trees-with-xgboost)

