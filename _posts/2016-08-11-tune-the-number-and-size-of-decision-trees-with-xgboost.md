---
layout: post
title: "Tune the Number and Size of Decision Trees with XGBoost"
date: 2016-08-11
categories: ['Machine Learning']
---

Gradient boosting involves the creation and addition of decision trees sequentially, each attempting to correct the mistakes of the learners that came before it.

This raises the question as to how many trees (weak learners or estimators) to configure in your gradient boosting model and how big each tree should be.

Here you will discover how to design a systematic experiment to select the number and size of decision trees to use on your problem.

By the end you will know:

- How to evaluate the effect of adding more decision trees to your XGBoost model?
- How to evaluate the effect of creating larger decision trees to your XGBoost model?
- How to investigate the relationship between the number and depth of trees on your problem?

[Source Code](https://github.com/srikanthpagadala/machine-learning-projects/tree/master/Tune%20the%20Number%20and%20Size%20of%20Decision%20Trees%20with%20XGBoost){:target="_blank"}

[Report](http://htmlpreview.github.io/?https://github.com/srikanthpagadala/machine-learning-projects/blob/master/Tune%20the%20Number%20and%20Size%20of%20Decision%20Trees%20with%20XGBoost/report.html){:target="_blank"}

Next: [Tune Learning Rate for Gradient Boosting with XGBoost](/notes/2016/08/12/tune-learning-rate-for-gradient-boosting-with-xgboost)

