---
layout: post
title: "Understanding Stateful LSTM Recurrent Neural Networks with Keras"
date: 2016-11-15
categories: ['Neural Networks']
---

A powerful and popular recurrent neural network is the long short-term model network or LSTM.

It is widely used because the architecture overcomes the vanishing and exploding gradient problem that plagues all recurrent neural networks, allowing very large and very deep networks to be created.

Like other recurrent neural networks, LSTM networks maintain state, and the specifics of how this is implemented in Keras framework can be confusing.

Here you will discover exactly how state is maintained in LSTM networks by the Keras deep learning library.

By the end you will know:

- How to develop a naive LSTM network for a sequence prediction problem?
- How to carefully manage state through batches and features with an LSTM network?
- Hot to manually manage state in an LSTM network for stateful prediction?

[Source Code](https://github.com/srikanthpagadala/neural-network-projects/tree/master/Understanding%20Stateful%20LSTM%20Recurrent%20Neural%20Networks%20with%20Keras){:target="_blank"}

[Report](http://htmlpreview.github.io/?https://github.com/srikanthpagadala/neural-network-projects/blob/master/Understanding%20Stateful%20LSTM%20Recurrent%20Neural%20Networks%20with%20Keras/report.html){:target="_blank"}

Next: [Text Generation with LSTM Recurrent Neural Networks with Keras](/notes/2016/11/16/text-generation-with-lstm-recurrent-neural-networks-with-keras)
