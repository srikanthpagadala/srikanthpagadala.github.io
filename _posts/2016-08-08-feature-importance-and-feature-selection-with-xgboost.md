---
layout: post
title: "Feature Importance and Feature Selection with XGBoost"
date: 2016-08-08
categories: ['Machine Learning']
---

A benefit of using ensembles of decision tree methods like gradient boosting is that they can automatically provide estimates of feature importance from a trained predictive model.

Here you will discover how you can estimate the importance of features for a predictive modeling problem using the XGBoost library in Python.

By the end you will know:

- How feature importance is calculated using the gradient boosting algorithm?
- How to plot feature importance in Python calculated by the XGBoost model?
- How to use feature importance calculated by XGBoost to perform feature selection?

[Source Code](https://github.com/srikanthpagadala/machine-learning-projects/tree/master/Feature%20Importance%20and%20Feature%20Selection%20With%20XGBoost){:target="_blank"}

[Report](http://htmlpreview.github.io/?https://github.com/srikanthpagadala/machine-learning-projects/blob/master/Feature%20Importance%20and%20Feature%20Selection%20With%20XGBoost/report.html){:target="_blank"}

Next: [Avoid Overfitting by Early Stopping with XGBoost](/notes/2016/08/09/avoid-overfitting-by-early-stopping-with-xgboost)

